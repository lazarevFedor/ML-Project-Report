{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Решение\n",
    "\n",
    "## Описание выбранного решения\n",
    "\n",
    "В качестве решения поставленной задачи был выбран подход _RAG_ (_Retrieval Augmented Generation_ — генерация с дополненной выборкой), который соединяет языковую модель с внешней базой знаний. Таким образом ИИ-помощник находит релевантные документы, ранжирует их и генерирует ответ на вопрос пользователя исходя из собственных знаний с опорой на найденные документы.\n",
    "\n",
    "Для реализации данного подхода были написаны следующие компоненты пайплайна:\n",
    "- Векторная база знаний с релевантными документами\n",
    "- Ретривер -- часть, отвечающая за извлечение релевантной информации из базы знаний исходя из запроса пользователя\n",
    "- Генератор -- часть, отвечающая за управление процессом генерации ответа языковой моделью\n",
    "- Языковая модель, формирующая ответы на запросы пользователя на основании входных данных из других компонент пайплайна\n",
    "\n",
    "Таким образом RAG-пайплайн включает в себя следующие шаги:\n",
    "1. **Индексация** — разрезание документов на чанки, преобразование их в векторное представление и сохранение в базе.\n",
    "2. **Поиск** — определение нужных чанков по запросу пользователя.\n",
    "3. **Подготовка** задания — формирование промта, соединяя запрос пользователя с найденными чанками, чтобы языковая модель получила контекст для генерации ответа.\n",
    "4. **Генерация** — создание ответ с помощью языковой модели.\n",
    "5. **Проверка** — контроль качества ответа, проверка его точности и отсутствия галлюцинаций. По необходимости можно добавить ссылки на источники данных.\n",
    "\n",
    "Также исходя из задачи, технических возможностей и архитектуры приложения _Wise Task_ был выбран микросервисный подход в архитектуре созданного приложения с разбиением RAG-пайплайна на микросервисы.\n",
    "\n",
    "В итоге, с учетом описанного выше, получилось реализовать микросервисное приложение состоящее из следующих логических частей:\n",
    "1. **Qdrant DB** -- векторная база знаний с релевантными документами про теорию графов и приложение _Wise Task_\n",
    "2. **Qdrant Indexer** -- сервис, который преобразует имеющиеся документы в чанки и выполняет их загрузку в **Qdrant DB**\n",
    "3. **Qdrant Ingest** -- сервис поиска релевантных документов в **Qdrant DB**\n",
    "4. **Core Service** -- сервис-оркестратор, управляет потоком данных в системе, взаимодействует с другими сервисами и базами данных, собирая все необходимое для ответа пользователю и обработки обратной связи от него\n",
    "5. **PostgreSQL Feedbacks** -- база данных для сбора обратной связи от пользователя о качестве ответа ИИ-помощника\n",
    "6. **llama_cpp** -- сервис на базе библиотеки llama.cpp с развернутой языковой моделью и сервером для приема запросов на генерацию\n",
    "7. **LLM Service** -- сервис для управления процессом генерации(построение промтов, настройка параметров, оценка метриками, оркестрация запросов на генерацию)\n",
    "\n",
    "### Обоснование выбора\n",
    "\n",
    "В качестве вариантов решения рассматривались следующие подходы:\n",
    "\n",
    "- Fine-tuning\n",
    "- RAG\n",
    "- LLM с длинным контекстом\n",
    "- Классический поиск\n",
    "\n",
    "Подход RAG был выбран так как он:\n",
    "- Доступ к актуальным данным без переобучения, так как оно является довольно дорогим и долгим с технической точки зрения. А эффективность такого подхода может варьироваться\n",
    "- Существенно сниженный риск галлюцинаций благодаря использованию проверенных источников, что особенно важно так как одним из основных требований к ответу является соответствие проверенным источникам.\n",
    "\n",
    "Минусы подхода RAG:\n",
    "- Качество ответа напрямую зависит от полноты, актуальности и релевантности извлечённых данных. При недостаточной подготовке данных возможно снижение точности.\n",
    "- Нужна подготовка данных для организации поискового индекса.\n",
    "\n",
    "Fine-tuning не был выбран из-за следующих факторов:\n",
    "- Долгое и ресурсоёмкое обучение, требует высокой экспертизы\n",
    "- Быстрое устаревание модели при изменении данных\n",
    "\n",
    "LLM с длинным контекстом не была выбрана из-за следующих факторов:\n",
    "- При большом объёме информации на входе не гарантируется точность ответа или то, что модель не запутается в фактах.\n",
    "- Длинный контекст также требует значительных ресурсов.\n",
    "\n",
    "Классический поиск не был выбран из-за следующих факторов:\n",
    "- Пользователь вручную обрабатывает результаты, что не подходит исходя из требований к ИИ-помощнику и исключает возможность объяснения непонятных пользователю тем.\n",
    "\n",
    "### LLM Service\n",
    "\n",
    "Данный сервис занимается подготовкой к генерации и управляет ее процессом.\n",
    "\n",
    "Путь запроса на генерацию состоит из следующих этапов:\n",
    "1. Поступление запроса на генерацию из Core Service`а, который включает вопрос и заданное количество контекстов.\n",
    "2. Выполняется определения типа вопроса с помощью классификатора.\n",
    "3. На основании типа вопроса строится промт включающий вопрос, пришедшие контексты и инструкции для модели.\n",
    "4. На основании типа вопроса задаются параметры генерации.\n",
    "5. Промт и параметры посылаются на llama_cpp.\n",
    "6. Модель генерирует ответ и отправляет результат.\n",
    "7. Выполняется постобработка ответа.\n",
    "8. Ответ отдается назад Core Service`у, который передает его наверх пользователю.\n",
    "\n",
    "#### llama.cpp\n",
    "\n",
    "В качестве языковой модели, используемой для генерации, была выбрана модель Qwen2.5:3B-Instruct, так как лучше всего показала себя в сравнении с другими исследуемыми моделями, что было выяснено экспериментальным путем. На сервере будет разворачиваться модель с большим количеством параметров.\n",
    "\n",
    "В качестве способа взаимодействия с моделью была выбрана библиотека llama.cpp, так как исходя из технических возможностей и требований к итоговому продукту был сделан упор на автономность и отказ от внешних зависимостей на сторонних сервисах предоставляющих API-ключи для взаимодействия с моделью и их ограничениях. Данная библиотека в свою очередь предоставляет возможность разворачивать модели локально с большим контролем процесса генерации. Также изначально рассматривался аналог -- фреймворк Ollama, но был осуществлен переход на llama.cpp для более точной настройки под конкретное железо и увеличения производительности и скорости генерации.\n",
    "\n",
    "#### Классификация вопросов\n",
    "\n",
    "Исходя из предметной области и ТЗ было выделено три типа приходящих вопросов:\n",
    "- Definition: вопрос, требующий ответа в виде точной формулировки(определение, теорема, лемма и т.д.)\n",
    "- Explanation: вопрос, требующий развернутого ответа с объяснением\n",
    "- Wise Task: вопрос про использование Wise Task\n",
    "\n",
    "Саму классификацию выполняет класс QueryClassifier, который определяет тип с помощью заранее заданных регулярных выражений. Если ни одно из регулярных выражений не подошло, присваивается тип Explanation, как наиболее общий."
   ],
   "id": "3ed59a334b2547c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "from logger import get_logger\n",
    "from patterns import Pattern\n",
    "\n",
    "\n",
    "class QueryClassifier:\n",
    "    \"\"\"Classifier for query type detection\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the QueryClassifier.\n",
    "\n",
    "        Sets up the logger for the classifier.\n",
    "        \"\"\"\n",
    "        self.logger = get_logger(__name__)\n",
    "\n",
    "    def classify(self, question: str):\n",
    "        \"\"\"\n",
    "        Detects query type based on predefined patterns in the question.\n",
    "\n",
    "        Analyzes the input question using regular expression patterns to determine\n",
    "        if it is seeking a 'definition' or an 'explanation'. Defaults to 'explanation'\n",
    "        if no patterns match.\n",
    "\n",
    "        Args:\n",
    "            question (str): The input question to classify.\n",
    "\n",
    "        Returns:\n",
    "            Literal['definition', 'explanation']: The detected query type.\n",
    "        \"\"\"\n",
    "        question_lower = question.lower().strip()\n",
    "        self.logger.debug(f'Classifying question: \"{question}\"')\n",
    "\n",
    "        for pattern in Pattern.wisetask_patterns:\n",
    "            if re.search(pattern, question_lower):\n",
    "                self.logger.debug(f'Question classified as WISE_TASK: \"{question}\"')\n",
    "                return 'wise_task'\n",
    "\n",
    "        for pattern in Pattern.definition_patterns:\n",
    "            if re.search(pattern, question_lower):\n",
    "                self.logger.debug(f'Question classified as DEFINITION: \"{question}\"')\n",
    "                return 'definition'\n",
    "\n",
    "        for pattern in Pattern.explanation_patterns:\n",
    "            if re.search(pattern, question_lower):\n",
    "                self.logger.debug(f'Question classified as EXPLANATION: \"{question}\"')\n",
    "                return 'explanation'\n",
    "\n",
    "        self.logger.debug(f'Question classified as EXPLANATION (default): \"{question}\"')\n",
    "        return 'explanation'"
   ],
   "id": "16fdbe643e850f15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Промт-инжиниринг\n",
    "\n",
    "Для каждого типа вопроса был составлен промт, чтобы точнее проинструктировать модель и получть более релевантный ответ.\n",
    "\n",
    "Промт для вопросов типа Wise Task:\n",
    "\n",
    "\n",
    "Промт для вопросов типа Explanation:\n",
    "\n",
    "\n",
    "Промт для вопросов типа Definition:\n",
    "\n",
    "#### Параметры генерации\n",
    "\n",
    "Были заданы следующие параметры генерации для вопросов типа Definition и Wise Task:"
   ],
   "id": "da054bd3e20ccead"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "temp = 0.3\n",
    "top_p = 0.5\n",
    "top_k = 20\n",
    "repeat_penalty = 1.05\n",
    "num_predict = 96"
   ],
   "id": "28600bc306c799cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Были заданы следующие параметры генерации для вопросов типа Explanation:",
   "id": "d6ae08c06044c4eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "temp = 0.5\n",
    "top_p = 0.7\n",
    "top_k = 40\n",
    "repeat_penalty = 1.05\n",
    "num_predict = 320"
   ],
   "id": "ef36f5d9113d9af2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Также для каждого из типов вопросов были выбраны заданы следующие параметры:",
   "id": "2d285b9552dfec2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "seed = 42\n",
    "num_ctx = 4096"
   ],
   "id": "524b1746253e2302"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Параметры запуска модели. Оптимизация скорости и качества генерации.\n",
    "\n",
    "Для оптимизации скорости генерации ответов и их качества, были настроены следующие параметры модели:\n",
    "\n",
    "Производительность модели\n",
    "- mlock\n",
    "- flash-attn\n",
    "- parallel N\n",
    "- cont-batching\n",
    "\n",
    "Кэширование запросов\n",
    "- slot-prompt-similarity\n",
    "- cache-ram\n",
    "\n",
    "Улучшение качества ответов в RAG\n",
    "- no-context-shift\n",
    "\n",
    "Приведенные выше параметры использовались при локальном тестировании и настройки модели, при развертывании на сервере будут внесены следующие изменения(\"+\" - параметр добавлен, \"-\" - параметр убран):\n",
    "\n",
    "- gpu-layers +\n",
    "- ubatch-size +\n",
    "- mlock -\n",
    "\n",
    "### Исследование решения\n",
    "\n",
    "#### Метрики для оценки качества ответов модели\n",
    "\n",
    "##### ROUGE-K (K = 1, 2)\n",
    "\n",
    "ROUGE — это набор метрик для оценки качества текста, которые сравнивают машинный результат с эталонным текстом. ROUGE фокусируется на recall — то есть насколько много информации из эталона содержится в машинном результате.\n",
    "\n",
    "- ROUGE-1 считает совпадения отдельных слов (unigrams) между кандидатом и эталоном.\n",
    "- ROUGE-2 Считает совпадения пар последовательно идущих слов (bigrams)."
   ],
   "id": "286d6e610a34324e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "def calculate_rouge_scores(reference, candidate):\n",
    "    \"\"\"\n",
    "    Calculate ROUGE scores for summarization evaluation\n",
    "\n",
    "    Args:\n",
    "        reference: Reference summary text\n",
    "        candidate: Generated summary text\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with ROUGE-1, ROUGE-2, and ROUGE-L scores\n",
    "    \"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(\n",
    "        ['rouge1', 'rouge2'],\n",
    "        use_stemmer=True\n",
    "    )\n",
    "\n",
    "    # Calculate scores\n",
    "    scores = scorer.score(reference, candidate)\n",
    "\n",
    "    # Extract F1 scores for each metric\n",
    "    results = {\n",
    "        'rouge1_f1': scores['rouge1'].fmeasure,\n",
    "        'rouge2_f1': scores['rouge2'].fmeasure,\n",
    "    }\n",
    "\n",
    "    return results"
   ],
   "id": "4db3e9797e5035c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### BLEU Score\n",
    "\n",
    "BLEU — это метрика оценки качества машинного перевода, которая основывается на сравнении отдельных слов и их сочетаний в переводе и эталонном тексте. Сначала считается, сколько таких совпадений есть в переводе, и это число делится на общее количество слов и сочетаний в самом переводе — получая так называемую точность. Чтобы не завышать оценку для слишком коротких или неполных переводов, к результату применяется поправка — штраф за краткость."
   ],
   "id": "92793a3c86562295"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "\n",
    "def calculate_bleu_score(reference, candidate):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for translation evaluation\n",
    "\n",
    "    Args:\n",
    "        reference: List of reference sentences (tokenized)\n",
    "        candidate: Generated sentence (tokenized)\n",
    "\n",
    "    Returns:\n",
    "        BLEU score between 0 and 1\n",
    "    \"\"\"\n",
    "    # Apply smoothing to handle zero n-gram matches\n",
    "    smoothing = SmoothingFunction().method4\n",
    "\n",
    "    # Calculate BLEU with 1-4 gram precision\n",
    "    bleu_score = sentence_bleu(\n",
    "        [reference],\n",
    "        candidate,\n",
    "        smoothing_function=smoothing\n",
    "    )\n",
    "\n",
    "    return bleu_score"
   ],
   "id": "36070f62a8c9d46f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### BERT Score\n",
    "\n",
    "BERTScore — метрика оценки качества текста, которая использует предобученные языковые модели, такие как BERT, для более точного сравнения кандидат-текста с эталонным."
   ],
   "id": "a00c2f969c755a23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from bert_score import score\n",
    "\n",
    "\n",
    "def calculate_bertscore(references, candidates, model_type='distilbert-base-uncased'):\n",
    "    \"\"\"\n",
    "    Calculate BERTScore for semantic similarity evaluation\n",
    "\n",
    "    Args:\n",
    "        references: List of reference texts\n",
    "        candidates: List of generated texts\n",
    "        model_type: BERT model for embedding computation\n",
    "\n",
    "    Returns:\n",
    "        Precision, Recall, and F1 BERTScores\n",
    "    \"\"\"\n",
    "\n",
    "    P, R, F1 = score(\n",
    "        candidates,\n",
    "        references,\n",
    "        model_type=model_type,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        'precision': P.tolist(),\n",
    "        'recall': R.tolist(),\n",
    "        'f1': F1.tolist()\n",
    "    }\n",
    "\n",
    "    return results"
   ],
   "id": "c57bb3d6c03b59e1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
