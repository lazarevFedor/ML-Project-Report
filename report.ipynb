{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Решение\n",
    "\n",
    "## Описание выбранного решения\n",
    "\n",
    "В качестве решения поставленной задачи был выбран подход _RAG_ (_Retrieval Augmented Generation_ — генерация с дополненной выборкой), который соединяет языковую модель с внешней базой знаний. Таким образом ИИ-помощник находит релевантные документы, ранжирует их и генерирует ответ на вопрос пользователя исходя из собственных знаний с опорой на найденные документы.\n",
    "\n",
    "Для реализации данного подхода были написаны следующие компоненты пайплайна:\n",
    "- Векторная база знаний с релевантными документами\n",
    "- Ретривер -- часть, отвечающая за извлечение релевантной информации из базы знаний исходя из запроса пользователя\n",
    "- Генератор -- часть, отвечающая за управление процессом генерации ответа языковой моделью\n",
    "- Языковая модель, формирующая ответы на запросы пользователя на основании входных данных из других компонент пайплайна\n",
    "\n",
    "Таким образом RAG-пайплайн включает в себя следующие шаги:\n",
    "1. **Индексация** — разрезание документов на чанки, преобразование их в векторное представление и сохранение в базе.\n",
    "2. **Поиск** — определение нужных чанков по запросу пользователя.\n",
    "3. **Подготовка** задания — формирование промта, соединяя запрос пользователя с найденными чанками, чтобы языковая модель получила контекст для генерации ответа.\n",
    "4. **Генерация** — создание ответ с помощью языковой модели.\n",
    "5. **Проверка** — контроль качества ответа, проверка его точности и отсутствия галлюцинаций. По необходимости можно добавить ссылки на источники данных.\n",
    "\n",
    "Также исходя из задачи, технических возможностей и архитектуры приложения _Wise Task_ был выбран микросервисный подход в архитектуре созданного приложения с разбиением RAG-пайплайна на микросервисы.\n",
    "\n",
    "В итоге, с учетом описанного выше, получилось реализовать микросервисное приложение состоящее из следующих логических частей:\n",
    "1. **Qdrant DB** -- векторная база знаний с релевантными документами про теорию графов и приложение _Wise Task_\n",
    "2. **Qdrant Indexer** -- сервис, который преобразует имеющиеся документы в чанки и выполняет их загрузку в **Qdrant DB**\n",
    "3. **Qdrant Ingest** -- сервис поиска релевантных документов в **Qdrant DB**\n",
    "4. **Core Service** -- сервис-оркестратор, управляет потоком данных в системе, взаимодействует с другими сервисами и базами данных, собирая все необходимое для ответа пользователю и обработки обратной связи от него\n",
    "5. **PostgreSQL Feedbacks** -- база данных для сбора обратной связи от пользователя о качестве ответа ИИ-помощника\n",
    "6. **llama_cpp** -- сервис на базе библиотеки llama.cpp с развернутой языковой моделью и сервером для приема запросов на генерацию\n",
    "7. **LLM Service** -- сервис для управления процессом генерации(построение промтов, настройка параметров, оценка метриками, оркестрация запросов на генерацию)\n",
    "\n",
    "### Обоснование выбора\n",
    "\n",
    "В качестве вариантов решения рассматривались следующие подходы:\n",
    "\n",
    "- Fine-tuning\n",
    "- RAG\n",
    "- LLM с длинным контекстом\n",
    "- Классический поиск\n",
    "\n",
    "### LLM Service\n",
    "\n",
    "Данный сервис занимается подготовкой к генерации и управляет ее процессом.\n",
    "\n",
    "Путь запроса на генерацию состоит из следующих этапов:\n",
    "1. Поступление запроса на генерацию из Core Service`а, который включает вопрос и заданное количество контекстов.\n",
    "2. Выполняется определения типа вопроса с помощью классификатора.\n",
    "3. На основании типа вопроса строится промт включающий вопрос, пришедшие контексты и инструкции для модели.\n",
    "4. На основании типа вопроса задаются параметры генерации.\n",
    "5. Промт и параметры посылаются на llama_cpp.\n",
    "6. Модель генерирует ответ и отправляет результат.\n",
    "7. Выполняется постобработка ответа.\n",
    "8. Ответ отдается назад Core Service`у, который передает его наверх пользователю.\n",
    "\n",
    "#### llama.cpp\n",
    "\n",
    "В качестве языковой модели, используемой для генерации, была выбрана модель Qwen2.5:3B-Instruct, так как лучше всего показала себя в сравнении с другими исследуемыми моделями, что было выяснено экспериментальным путем. На сервере будет разворачиваться модель с большим количеством параметров.\n",
    "\n",
    "В качестве способа взаимодействия с моделью была выбрана библиотека llama.cpp, так как исходя из технических возможностей и требований к итоговому продукту был сделан упор на автономность и отказ от внешних зависимостей на сторонних сервисах предоставляющих API-ключи для взаимодействия с моделью и их ограничениях. Данная библиотека в свою очередь предоставляет возможность разворачивать модели локально с большим контролем процесса генерации. Также изначально рассматривался аналог -- фреймворк Ollama, но был осуществлен переход на llama.cpp для более точной настройки под конкретное железо и увеличения производительности и скорости генерации.\n",
    "\n",
    "#### Классификация вопросов\n",
    "\n",
    "Исходя из предметной области и ТЗ было выделено три типа приходящих вопросов:\n",
    "- Definition: вопрос, требующий ответа в виде точной формулировки(определение, теорема, лемма и т.д.)\n",
    "- Explanation: вопрос, требующий развернутого ответа с объяснением\n",
    "- Wise Task: вопрос про использование Wise Task\n",
    "\n",
    "Саму классификацию выполняет класс QueryClassifier, который определяет тип с помощью заранее заданных регулярных выражений. Если ни одно из регулярных выражений не подошло, присваивается тип Explanation, как наиболее общий."
   ],
   "id": "3ed59a334b2547c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "from logger import get_logger\n",
    "from patterns import Pattern\n",
    "\n",
    "\n",
    "class QueryClassifier:\n",
    "    \"\"\"Classifier for query type detection\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the QueryClassifier.\n",
    "\n",
    "        Sets up the logger for the classifier.\n",
    "        \"\"\"\n",
    "        self.logger = get_logger(__name__)\n",
    "\n",
    "    def classify(self, question: str):\n",
    "        \"\"\"\n",
    "        Detects query type based on predefined patterns in the question.\n",
    "\n",
    "        Analyzes the input question using regular expression patterns to determine\n",
    "        if it is seeking a 'definition' or an 'explanation'. Defaults to 'explanation'\n",
    "        if no patterns match.\n",
    "\n",
    "        Args:\n",
    "            question (str): The input question to classify.\n",
    "\n",
    "        Returns:\n",
    "            Literal['definition', 'explanation']: The detected query type.\n",
    "        \"\"\"\n",
    "        question_lower = question.lower().strip()\n",
    "        self.logger.debug(f'Classifying question: \"{question}\"')\n",
    "\n",
    "        for pattern in Pattern.wisetask_patterns:\n",
    "            if re.search(pattern, question_lower):\n",
    "                self.logger.debug(f'Question classified as WISE_TASK: \"{question}\"')\n",
    "                return 'wise_task'\n",
    "\n",
    "        for pattern in Pattern.definition_patterns:\n",
    "            if re.search(pattern, question_lower):\n",
    "                self.logger.debug(f'Question classified as DEFINITION: \"{question}\"')\n",
    "                return 'definition'\n",
    "\n",
    "        for pattern in Pattern.explanation_patterns:\n",
    "            if re.search(pattern, question_lower):\n",
    "                self.logger.debug(f'Question classified as EXPLANATION: \"{question}\"')\n",
    "                return 'explanation'\n",
    "\n",
    "        self.logger.debug(f'Question classified as EXPLANATION (default): \"{question}\"')\n",
    "        return 'explanation'"
   ],
   "id": "16fdbe643e850f15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Промт-инжиниринг\n",
    "\n",
    "Для каждого типа вопроса был составлен промт, чтобы точнее проинструктировать модель и получть более релевантный ответ.\n",
    "\n",
    "Промт для вопросов типа Wise Task:\n",
    "\n",
    "\n",
    "Промт для вопросов типа Explanation:\n",
    "\n",
    "\n",
    "Промт для вопросов типа Definition:\n",
    "\n",
    "#### Параметры генерации\n",
    "\n",
    "Были заданы следующие параметры генерации для вопросов типа Definition и Wise Task:"
   ],
   "id": "da054bd3e20ccead"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "temp = 0.3\n",
    "top_p = 0.5\n",
    "top_k = 20\n",
    "repeat_penalty = 1.05\n",
    "num_predict = 96"
   ],
   "id": "28600bc306c799cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Были заданы следующие параметры генерации для вопросов типа Explanation:",
   "id": "d6ae08c06044c4eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "temp = 0.5\n",
    "top_p = 0.7\n",
    "top_k = 40\n",
    "repeat_penalty = 1.05\n",
    "num_predict = 320"
   ],
   "id": "ef36f5d9113d9af2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Также для каждого из типов вопросов были выбраны заданы следующие параметры:",
   "id": "2d285b9552dfec2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "seed = 42\n",
    "num_ctx = 4096"
   ],
   "id": "524b1746253e2302"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Параметры запуска модели. Оптимизация скорости и качества генерации.\n",
    "\n",
    "Для оптимизации скорости генерации ответов и их качества, были настроены следующие параметры модели:\n",
    "\n",
    "Производительность модели\n",
    "- mlock\n",
    "- flash-attn\n",
    "- parallel N\n",
    "- cont-batching\n",
    "\n",
    "Кэширование запросов\n",
    "- slot-prompt-similarity\n",
    "- cache-ram\n",
    "\n",
    "Улучшение качества ответов в RAG\n",
    "- no-context-shift\n",
    "\n",
    "Приведенные выше параметры использовались при локальном тестировании и настройки модели, при развертывании на сервере будут внесены следующие изменения(\"+\" - параметр добавлен, \"-\" - параметр убран):\n",
    "\n",
    "- gpu-layers +\n",
    "- ubatch-size +\n",
    "- mlock -"
   ],
   "id": "286d6e610a34324e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
