{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Введение и постановка задачи\n",
    "## 1.1 Введение\n",
    "\n",
    "Современные образовательные веб-платформы всё чаще используют интеллектуальные системы поддержки пользователей, позволяющие ускорить поиск информации и повысить качество усвоения учебного материала. Особенно актуально применение таких систем в технических дисциплинах, включая теорию графов и методы машинного обучения, где обучающимся требуется оперативный доступ к точным и формально корректным теоретическим объяснениям.\n",
    "\n",
    "Сайт Wise Task представляет собой учебную платформу, предназначенную для практики по теории графов и смежным разделам математики.\n",
    "\n",
    "Для повышения удобства работы с учебным контентом целесообразно использовать интеллектуального чат-помощника, способного отвечать на вопросы пользователей на основе строго ограниченного корпуса учебных материалов. В рамках данного проекта для реализации такого помощника применяется подход Retrieval-Augmented Generation (RAG) с использованием векторной базы данных QdrantDB и языковой модели Qwen.\n",
    "\n",
    "## 1.2 Постановка задачи\n",
    "\n",
    "Целью работы является разработка чат-помощника для сайта Wise Task, предназначенного для предоставления точных и обоснованных ответов по теории графов и другим учебным материалам, загруженным в систему.\n",
    "\n",
    "Для достижения поставленной цели необходимо решить следующие задачи:\n",
    "\n",
    "разработать архитектуру чат-помощника на основе подхода Retrieval-Augmented Generation;\n",
    "\n",
    "подготовить и загрузить учебные материалы по теории графов и машинному обучению во векторную базу данных;\n",
    "\n",
    "реализовать механизм семантического поиска релевантных фрагментов теории по пользовательскому запросу;\n",
    "\n",
    "интегрировать языковую модель Qwen для генерации ответов с использованием найденного контекста;\n",
    "\n",
    "обеспечить формирование ответов исключительно на основе загруженных в систему данных;\n",
    "\n",
    "реализовать пользовательский интерфейс чат-помощника для взаимодействия с посетителями сайта Wise Task;\n",
    "\n",
    "провести тестирование корректности и полноты ответов системы.\n",
    "\n",
    "Результатом работы должен стать чат-помощник, повышающий эффективность работы пользователей с учебными материалами сайта Wise Task и способствующий более глубокому пониманию теории графов и методов машинного обучения."
   ],
   "id": "f761a1279641457"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Решение\n",
    "\n",
    "## Описание выбранного решения\n",
    "\n",
    "В качестве решения поставленной задачи был выбран подход _RAG_ (_Retrieval Augmented Generation_ — генерация с дополненной выборкой), который соединяет языковую модель с внешней базой знаний. Таким образом ИИ-помощник находит релевантные документы, ранжирует их и генерирует ответ на вопрос пользователя исходя из собственных знаний с опорой на найденные документы.\n",
    "\n",
    "Для реализации данного подхода были написаны следующие компоненты пайплайна:\n",
    "- Векторная база знаний с релевантными документами\n",
    "- Ретривер -- часть, отвечающая за извлечение релевантной информации из базы знаний исходя из запроса пользователя\n",
    "- Генератор -- часть, отвечающая за управление процессом генерации ответа языковой моделью\n",
    "- Языковая модель, формирующая ответы на запросы пользователя на основании входных данных из других компонент пайплайна\n",
    "\n",
    "Таким образом RAG-пайплайн включает в себя следующие шаги:\n",
    "1. **Индексация** — разрезание документов на чанки, преобразование их в векторное представление и сохранение в базе.\n",
    "2. **Поиск** — определение нужных чанков по запросу пользователя.\n",
    "3. **Подготовка** задания — формирование промта, соединяя запрос пользователя с найденными чанками, чтобы языковая модель получила контекст для генерации ответа.\n",
    "4. **Генерация** — создание ответ с помощью языковой модели.\n",
    "5. **Проверка** — контроль качества ответа, проверка его точности и отсутствия галлюцинаций. По необходимости можно добавить ссылки на источники данных.\n",
    "\n",
    "Также исходя из задачи, технических возможностей и архитектуры приложения _Wise Task_ был выбран микросервисный подход в архитектуре созданного приложения с разбиением RAG-пайплайна на микросервисы.\n",
    "\n",
    "В итоге, с учетом описанного выше, получилось реализовать микросервисное приложение состоящее из следующих логических частей:\n",
    "1. **Qdrant DB** -- векторная база знаний с релевантными документами про теорию графов и приложение _Wise Task_\n",
    "2. **Qdrant Indexer** -- сервис, который преобразует имеющиеся документы в чанки и выполняет их загрузку в **Qdrant DB**\n",
    "3. **Qdrant Ingest** -- сервис поиска релевантных документов в **Qdrant DB**\n",
    "4. **Core Service** -- сервис-оркестратор, управляет потоком данных в системе, взаимодействует с другими сервисами и базами данных, собирая все необходимое для ответа пользователю и обработки обратной связи от него\n",
    "5. **PostgreSQL Feedbacks** -- база данных для сбора обратной связи от пользователя о качестве ответа ИИ-помощника\n",
    "6. **llama_cpp** -- сервис на базе библиотеки llama.cpp с развернутой языковой моделью и сервером для приема запросов на генерацию\n",
    "7. **LLM Service** -- сервис для управления процессом генерации(построение промтов, настройка параметров, оценка метриками, оркестрация запросов на генерацию)\n",
    "\n",
    "### Обоснование выбора\n",
    "\n",
    "В качестве вариантов решения рассматривались следующие подходы:\n",
    "\n",
    "- Fine-tuning\n",
    "- RAG\n",
    "- LLM с длинным контекстом\n",
    "- Классический поиск\n",
    "\n",
    "Подход RAG был выбран так как он:\n",
    "- Доступ к актуальным данным без переобучения, так как оно является довольно дорогим и долгим с технической точки зрения. А эффективность такого подхода может варьироваться\n",
    "- Существенно сниженный риск галлюцинаций благодаря использованию проверенных источников, что особенно важно так как одним из основных требований к ответу является соответствие проверенным источникам.\n",
    "\n",
    "Минусы подхода RAG:\n",
    "- Качество ответа напрямую зависит от полноты, актуальности и релевантности извлечённых данных. При недостаточной подготовке данных возможно снижение точности.\n",
    "- Нужна подготовка данных для организации поискового индекса.\n",
    "\n",
    "Fine-tuning не был выбран из-за следующих факторов:\n",
    "- Долгое и ресурсоёмкое обучение, требует высокой экспертизы\n",
    "- Быстрое устаревание модели при изменении данных\n",
    "\n",
    "LLM с длинным контекстом не была выбрана из-за следующих факторов:\n",
    "- При большом объёме информации на входе не гарантируется точность ответа или то, что модель не запутается в фактах.\n",
    "- Длинный контекст также требует значительных ресурсов.\n",
    "\n",
    "Классический поиск не был выбран из-за следующих факторов:\n",
    "- Пользователь вручную обрабатывает результаты, что не подходит исходя из требований к ИИ-помощнику и исключает возможность объяснения непонятных пользователю тем.\n",
    "\n",
    "### Реализация Retriever’а\n",
    "\n",
    "В рамках проекта был реализован специализированный retriever, основанный на гибридном поиске, объединяющем семантический векторный поиск и классические методы информационного поиска. Данный подход позволяет компенсировать ограничения каждого из методов по отдельности и повысить общее качество извлечения релевантных фрагментов.\n",
    "\n",
    "Retriever реализован в виде отдельного сервиса и инкапсулирован в классе Searcher, отвечающем за полный цикл обработки пользовательского запроса — от его векторизации до финального ранжирования результатов.\n",
    "\n",
    "Векторный поиск\n",
    "\n",
    "В качестве основы семантического поиска используется векторная база данных QdrantDB. Для каждого пользовательского запроса выполняются следующие действия:\n",
    "\n",
    "Запрос преобразуется в векторное представление с помощью модели эмбеддингов\n",
    "sentence-transformers/paraphrase-multilingual-mpnet-base-v2,\n",
    "что обеспечивает поддержку многоязычных запросов, включая русский язык.\n",
    "\n",
    "Полученный embedding используется для поиска ближайших векторов в коллекции Qdrant с применением точного поиска (exact search).\n",
    "\n",
    "В результате возвращается расширенный список кандидатов (с запасом), который далее используется для дополнительного ранжирования.\n",
    "\n",
    "Использование точного поиска оправдано сравнительно умеренным объёмом данных и позволяет получить максимально корректную оценку семантической близости.\n",
    "\n",
    "Лексический поиск на основе BM25\n",
    "\n",
    "Для повышения качества извлечения релевантных фрагментов дополнительно используется BM25-поиск, реализованный поверх предварительно сохранённого индекса. Индекс загружается при инициализации retriever’а и используется для вычисления лексической релевантности запроса.\n",
    "\n",
    "Особенности реализации:\n",
    "\n",
    "используется токенизация с нормализацией и очисткой текста;\n",
    "\n",
    "BM25-оценки нормализуются относительно максимального значения;\n",
    "\n",
    "при отсутствии индекса система автоматически переходит в режим чисто векторного поиска.\n",
    "\n",
    "Таким образом обеспечивается устойчивость системы к неполноте данных и деградации отдельных компонентов.\n",
    "\n",
    "Гибридное ранжирование результатов\n",
    "\n",
    "Для объединения результатов векторного и BM25-поиска используется взвешенная гибридная метрика:\n",
    "\n",
    "score hybrid = α⋅score vector +(1−α)⋅score_BM25\n",
    "\n",
    "Такой подход позволяет учитывать как смысловую близость запроса и документа, так и наличие точных совпадений терминов.\n",
    "\n",
    "Правила дополнительного скоринга\n",
    "\n",
    "После вычисления гибридной оценки применяется набор эвристических правил, направленных на повышение качества итогового ранжирования:\n",
    "\n",
    "учитывается наличие ключевых слов запроса в заголовке документа;\n",
    "\n",
    "повышается вес фрагментов, содержащих маркеры формальных элементов\n",
    "(например: определение, теорема, лемма, алгоритм);\n",
    "\n",
    "понижается вес секций справочного характера\n",
    "(литература, источники, см. также);\n",
    "\n",
    "применяется штраф для фрагментов с высокой плотностью ссылок;\n",
    "\n",
    "учитывается наличие точных подстрочных совпадений и n-грамм (bi- и tri-грамм).\n",
    "\n",
    "Дополнительно реализована проверка наличия пересечений слов запроса с текстом фрагмента — при полном отсутствии таких пересечений итоговая оценка существенно понижается.\n",
    "\n",
    "Итоговый алгоритм работы Retriever’а\n",
    "\n",
    "В общем виде алгоритм работы retriever’а можно описать следующим образом:\n",
    "\n",
    "Векторизация пользовательского запроса.\n",
    "\n",
    "Получение кандидатов с помощью семантического поиска в QdrantDB.\n",
    "\n",
    "Вычисление BM25-оценок для всего корпуса документов.\n",
    "\n",
    "Объединение оценок в гибридную метрику.\n",
    "\n",
    "Применение набора эвристических правил скоринга.\n",
    "\n",
    "Сортировка результатов по итоговой оценке.\n",
    "\n",
    "Возврат топ-k наиболее релевантных фрагментов.\n",
    "\n",
    "Такой многоэтапный процесс извлечения позволяет существенно повысить точность контекста, передаваемого языковой модели, и снизить вероятность генерации некорректных или нерелевантных ответов.\n",
    "\n",
    "### LLM Service\n",
    "\n",
    "Данный сервис занимается подготовкой к генерации и управляет ее процессом.\n",
    "\n",
    "Путь запроса на генерацию состоит из следующих этапов:\n",
    "1. Поступление запроса на генерацию из Core Service`а, который включает вопрос и заданное количество контекстов.\n",
    "2. Выполняется определения типа вопроса с помощью классификатора.\n",
    "3. На основании типа вопроса строится промт включающий вопрос, пришедшие контексты и инструкции для модели.\n",
    "4. На основании типа вопроса задаются параметры генерации.\n",
    "5. Промт и параметры посылаются на llama_cpp.\n",
    "6. Модель генерирует ответ и отправляет результат.\n",
    "7. Выполняется постобработка ответа.\n",
    "8. Ответ отдается назад Core Service`у, который передает его наверх пользователю.\n",
    "\n",
    "#### llama.cpp\n",
    "\n",
    "В качестве языковой модели, используемой для генерации, была выбрана модель Qwen2.5:3B-Instruct, так как лучше всего показала себя в сравнении с другими исследуемыми моделями, что было выяснено экспериментальным путем. На сервере будет разворачиваться модель с большим количеством параметров.\n",
    "\n",
    "В качестве способа взаимодействия с моделью была выбрана библиотека llama.cpp, так как исходя из технических возможностей и требований к итоговому продукту был сделан упор на автономность и отказ от внешних зависимостей на сторонних сервисах предоставляющих API-ключи для взаимодействия с моделью и их ограничениях. Данная библиотека в свою очередь предоставляет возможность разворачивать модели локально с большим контролем процесса генерации. Также изначально рассматривался аналог -- фреймворк Ollama, но был осуществлен переход на llama.cpp для более точной настройки под конкретное железо и увеличения производительности и скорости генерации.\n",
    "\n",
    "#### Классификация вопросов\n",
    "\n",
    "Исходя из предметной области и ТЗ было выделено три типа приходящих вопросов:\n",
    "- Definition: вопрос, требующий ответа в виде точной формулировки(определение, теорема, лемма и т.д.)\n",
    "- Explanation: вопрос, требующий развернутого ответа с объяснением\n",
    "- Wise Task: вопрос про использование Wise Task\n",
    "\n",
    "Саму классификацию выполняет класс QueryClassifier, который определяет тип с помощью заранее заданных регулярных выражений. Если ни одно из регулярных выражений не подошло, присваивается тип Explanation, как наиболее общий."
   ],
   "id": "3ed59a334b2547c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T08:36:58.134883600Z",
     "start_time": "2025-12-25T08:36:57.742169200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from logger import get_logger\n",
    "from patterns import Pattern\n",
    "\n",
    "\n",
    "class QueryClassifier:\n",
    "    \"\"\"Classifier for query type detection\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the QueryClassifier.\n",
    "\n",
    "        Sets up the logger for the classifier.\n",
    "        \"\"\"\n",
    "        self.logger = get_logger(__name__)\n",
    "\n",
    "    def classify(self, question: str):\n",
    "        \"\"\"\n",
    "        Detects query type based on predefined patterns in the question.\n",
    "\n",
    "        Analyzes the input question using regular expression patterns to determine\n",
    "        if it is seeking a 'definition' or an 'explanation'. Defaults to 'explanation'\n",
    "        if no patterns match.\n",
    "\n",
    "        Args:\n",
    "            question (str): The input question to classify.\n",
    "\n",
    "        Returns:\n",
    "            Literal['definition', 'explanation']: The detected query type.\n",
    "        \"\"\"\n",
    "        question_lower = question.lower().strip()\n",
    "        self.logger.debug(f'Classifying question: \"{question}\"')\n",
    "\n",
    "        for pattern in Pattern.wisetask_patterns:\n",
    "            if re.search(pattern, question_lower):\n",
    "                self.logger.debug(f'Question classified as WISE_TASK: \"{question}\"')\n",
    "                return 'wise_task'\n",
    "\n",
    "        for pattern in Pattern.definition_patterns:\n",
    "            if re.search(pattern, question_lower):\n",
    "                self.logger.debug(f'Question classified as DEFINITION: \"{question}\"')\n",
    "                return 'definition'\n",
    "\n",
    "        for pattern in Pattern.explanation_patterns:\n",
    "            if re.search(pattern, question_lower):\n",
    "                self.logger.debug(f'Question classified as EXPLANATION: \"{question}\"')\n",
    "                return 'explanation'\n",
    "\n",
    "        self.logger.debug(f'Question classified as EXPLANATION (default): \"{question}\"')\n",
    "        return 'explanation'"
   ],
   "id": "16fdbe643e850f15",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'logger'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mre\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlogger\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m get_logger\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpatterns\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Pattern\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mQueryClassifier\u001B[39;00m:\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'logger'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Промт-инжиниринг\n",
    "\n",
    "Для каждого типа вопроса был составлен промт, чтобы точнее проинструктировать модель и получть более релевантный ответ.\n",
    "\n",
    "Промт для вопросов типа Wise Task:\n",
    "Ответь на вопрос по тренажеру Wise Task, используй контекст для ответа\n",
    "\n",
    "Промт для вопросов типа Explanation:\n",
    "Ответь на вопрос и кратко объясни тему. Для ответа используй контекст.\n",
    "\n",
    "Промт для вопросов типа Definition:\n",
    "Ответь кратко на вопрос, используй контекст для ответа.\n",
    "\n",
    "#### Параметры генерации\n",
    "\n",
    "Были заданы следующие параметры генерации для вопросов типа Definition и Wise Task:"
   ],
   "id": "da054bd3e20ccead"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "temp = 0.3\n",
    "top_p = 0.5\n",
    "top_k = 20\n",
    "repeat_penalty = 1.05\n",
    "num_predict = 96"
   ],
   "id": "28600bc306c799cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Были заданы следующие параметры генерации для вопросов типа Explanation:",
   "id": "d6ae08c06044c4eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "temp = 0.5\n",
    "top_p = 0.7\n",
    "top_k = 40\n",
    "repeat_penalty = 1.05\n",
    "num_predict = 320"
   ],
   "id": "ef36f5d9113d9af2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Также для каждого из типов вопросов были выбраны заданы следующие параметры:",
   "id": "2d285b9552dfec2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "seed = 42\n",
    "num_ctx = 4096"
   ],
   "id": "524b1746253e2302",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Параметры запуска модели. Оптимизация скорости и качества генерации.\n",
    "\n",
    "Для оптимизации скорости генерации ответов и их качества, были настроены следующие параметры модели:\n",
    "\n",
    "Производительность модели\n",
    "- mlock\n",
    "- flash-attn\n",
    "- parallel N\n",
    "- cont-batching\n",
    "\n",
    "Кэширование запросов\n",
    "- slot-prompt-similarity\n",
    "- cache-ram\n",
    "\n",
    "Улучшение качества ответов в RAG\n",
    "- no-context-shift\n",
    "\n",
    "Приведенные выше параметры использовались при локальном тестировании и настройки модели, при развертывании на сервере будут внесены следующие изменения(\"+\" - параметр добавлен, \"-\" - параметр убран):\n",
    "\n",
    "- gpu-layers +\n",
    "- ubatch-size +\n",
    "- mlock -\n",
    "\n",
    "### Исследование решения\n",
    "\n",
    "#### Метрики для оценки качества ответов модели\n",
    "\n",
    "##### ROUGE-K (K = 1, 2)\n",
    "\n",
    "ROUGE — это набор метрик для оценки качества текста, которые сравнивают машинный результат с эталонным текстом. ROUGE фокусируется на recall — то есть насколько много информации из эталона содержится в машинном результате.\n",
    "\n",
    "- ROUGE-1 считает совпадения отдельных слов (unigrams) между кандидатом и эталоном.\n",
    "- ROUGE-2 Считает совпадения пар последовательно идущих слов (bigrams)."
   ],
   "id": "286d6e610a34324e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "def calculate_rouge_scores(reference, candidate):\n",
    "    \"\"\"\n",
    "    Calculate ROUGE scores for summarization evaluation\n",
    "\n",
    "    Args:\n",
    "        reference: Reference summary text\n",
    "        candidate: Generated summary text\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with ROUGE-1, ROUGE-2, and ROUGE-L scores\n",
    "    \"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(\n",
    "        ['rouge1', 'rouge2'],\n",
    "        use_stemmer=True\n",
    "    )\n",
    "\n",
    "    # Calculate scores\n",
    "    scores = scorer.score(reference, candidate)\n",
    "\n",
    "    # Extract F1 scores for each metric\n",
    "    results = {\n",
    "        'rouge1_f1': scores['rouge1'].fmeasure,\n",
    "        'rouge2_f1': scores['rouge2'].fmeasure,\n",
    "    }\n",
    "\n",
    "    return results"
   ],
   "id": "4db3e9797e5035c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### BLEU Score\n",
    "\n",
    "BLEU — это метрика оценки качества машинного перевода, которая основывается на сравнении отдельных слов и их сочетаний в переводе и эталонном тексте. Сначала считается, сколько таких совпадений есть в переводе, и это число делится на общее количество слов и сочетаний в самом переводе — получая так называемую точность. Чтобы не завышать оценку для слишком коротких или неполных переводов, к результату применяется поправка — штраф за краткость."
   ],
   "id": "92793a3c86562295"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "\n",
    "def calculate_bleu_score(reference, candidate):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for translation evaluation\n",
    "\n",
    "    Args:\n",
    "        reference: List of reference sentences (tokenized)\n",
    "        candidate: Generated sentence (tokenized)\n",
    "\n",
    "    Returns:\n",
    "        BLEU score between 0 and 1\n",
    "    \"\"\"\n",
    "    # Apply smoothing to handle zero n-gram matches\n",
    "    smoothing = SmoothingFunction().method4\n",
    "\n",
    "    # Calculate BLEU with 1-4 gram precision\n",
    "    bleu_score = sentence_bleu(\n",
    "        [reference],\n",
    "        candidate,\n",
    "        smoothing_function=smoothing\n",
    "    )\n",
    "\n",
    "    return bleu_score"
   ],
   "id": "36070f62a8c9d46f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### BERT Score\n",
    "\n",
    "BERTScore — метрика оценки качества текста, которая использует предобученные языковые модели, такие как BERT, для более точного сравнения кандидат-текста с эталонным."
   ],
   "id": "a00c2f969c755a23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from bert_score import score\n",
    "\n",
    "\n",
    "def calculate_bertscore(references, candidates, model_type='distilbert-base-uncased'):\n",
    "    \"\"\"\n",
    "    Calculate BERTScore for semantic similarity evaluation\n",
    "\n",
    "    Args:\n",
    "        references: List of reference texts\n",
    "        candidates: List of generated texts\n",
    "        model_type: BERT model for embedding computation\n",
    "\n",
    "    Returns:\n",
    "        Precision, Recall, and F1 BERTScores\n",
    "    \"\"\"\n",
    "\n",
    "    P, R, F1 = score(\n",
    "        candidates,\n",
    "        references,\n",
    "        model_type=model_type,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        'precision': P.tolist(),\n",
    "        'recall': R.tolist(),\n",
    "        'f1': F1.tolist()\n",
    "    }\n",
    "\n",
    "    return results"
   ],
   "id": "c57bb3d6c03b59e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Метрики и оценка качества работы Retriever’а\n",
    "\n",
    "Качество работы retriever’а напрямую влияет на итоговую точность ответов чат-помощника, поскольку языковая модель использует исключительно извлечённый контекст. В связи с этим для оценки эффективности retriever’а были рассмотрены как **количественные метрики информационного поиска**, так и **качественные пользовательские оценки**.\n",
    "\n",
    "\n",
    "\n",
    "### Количественные метрики релевантности\n",
    "\n",
    "Для оценки качества извлечения релевантных фрагментов использовались стандартные метрики, применяемые в задачах информационного поиска.\n",
    "\n",
    "#### Recall@k\n",
    "\n",
    "Метрика **Recall@k** показывает, присутствует ли хотя бы один релевантный фрагмент среди первых *k* результатов поиска.\n",
    "\n",
    "$$\n",
    "Recall@k = \\frac{\\text{количество запросов с релевантным документом в топ-}k}\n",
    "{\\text{общее количество запросов}}\n",
    "$$\n",
    "\n",
    "Данная метрика особенно важна в контексте RAG-систем, поскольку отсутствие релевантного контекста делает невозможной генерацию корректного ответа.\n",
    "\n",
    "\n",
    "\n",
    "#### Precision@k\n",
    "\n",
    "Метрика **Precision@k** отражает долю релевантных фрагментов среди первых *k* результатов.\n",
    "\n",
    "$$\n",
    "Precision@k = \\frac{\\text{количество релевантных фрагментов в топ-}k}{k}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Mean Reciprocal Rank (MRR)\n",
    "\n",
    "Метрика **MRR** оценивает позицию первого релевантного фрагмента в результатах поиска.\n",
    "\n",
    "$$\n",
    "MRR = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{rank_i}\n",
    "$$\n",
    "\n",
    "где $rank_i$ — позиция первого релевантного результата для *i*-го запроса.\n",
    "\n",
    "\n",
    "### Оценка гибридного поиска\n",
    "\n",
    "Отдельно анализировалось влияние гибридного подхода (векторный поиск + BM25) на качество извлечения. Для этого проводилось сравнение следующих режимов работы retriever’а:\n",
    "\n",
    "- только векторный поиск;\n",
    "- только BM25;\n",
    "- гибридный поиск с различными значениями параметра $\\alpha$.\n",
    "\n",
    "\n",
    "Сравнение выполнялось по метрикам **Recall@k** и **MRR**, что позволило эмпирически подобрать оптимальное значение параметра \\(\\alpha\\).\n",
    "\n",
    "\n",
    "\n",
    "### Оценка скорости выполнения\n",
    "\n",
    "Для оценки производительности retriever’а измерялись следующие показатели:\n",
    "\n",
    "- время векторизации запроса;\n",
    "- время поиска в QdrantDB;\n",
    "- время гибридного ранжирования;\n",
    "- общее время ответа retriever’а.\n",
    "\n",
    "В качестве основных метрик использовались:\n",
    "- среднее время отклика;\n",
    "- 95-й перцентиль времени ответа (p95 latency).\n",
    "\n",
    "\n",
    "### Выводы по результатам оценки\n",
    "\n",
    "Результаты экспериментов показали, что использование **гибридного retriever’а** обеспечивает более высокие значения **Recall@k** и **MRR** по сравнению с использованием только векторного или только лексического поиска. Также было отмечено снижение количества нерелевантных фрагментов, передаваемых языковой модели, что положительно влияет на качество финальных ответов чат-помощника.\n"
   ],
   "id": "5a3cef394be80535"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Список литературы\n",
    "\n",
    "1. https://yandex.cloud/ru/blog/posts/2025/05/retrieval-augmented-generation-basics?ysclid=mgg2z2jjuq477389471&utm_referrer=https%3A%2F%2Fyandex.ru%2F&utm_referrer=https%3A%2F%2Fyndx.auth.yandex.cloud%2F&utm_referrer=https%3A%2F%2Fyndx.auth.yandex.cloud%2F\n",
    "2. https://habr.com/ru/articles/870174/\n",
    "3. https://pixelfed.nbics.net/books/u-12-ollama/page/parametry-llama-cli\n",
    "4. https://dev.to/rmaurodev/running-llamacpp-in-docker-on-raspberry-pi-4g29\n",
    "5. https://llmstudio.ru/blog/bleu-rouge-bert-score?ysclid=mippcykuk7543577973\n",
    "6. https://yandex.cloud/ru/blog/posts/2025/05/retrieval-augmented-generation-basics?ysclid=mgg2z2jjuq477389471&utm_referrer=https%3A%2F%2Fyandex.ru%2F&utm_referrer=https%3A%2F%2Fyndx.auth.yandex.cloud%2F&utm_referrer=https%3A%2F%2Fyndx.auth.yandex.cloud%2F\n",
    "7. https://yandex.cloud/ru/blog/posts/2025/03/fine-tuning?utm_referrer=https%3A%2F%2Fyandex.cloud%2Fru%2Fblog%2Fposts%2F2025%2F05%2Fretrieval-augmented-generation-basics%3Fysclid%3Dmgg2z2jjuq477389471%26utm_referrer%3Dhttps%253A%252F%252Fyandex.ru%252F&utm_referrer=https%3A%2F%2Fyndx.auth.yandex.cloud%2F\n",
    "8. https://docs.claude.com/en/docs/build-with-claude/prompt-engineering/use-xml-tags\n",
    "9. https://www.promptingguide.ai/techniques"
   ],
   "id": "e031ce126e3a8896"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Заключение\n",
    "\n",
    "В рамках данной работы была поставлена и решена задача разработки интеллектуального чат-помощника для учебного сайта **Wise Task**.\n",
    "\n",
    "Основной целью являлось создание системы, способной предоставлять точные, контекстно обоснованные и проверяемые ответы на основе заранее подготовленного корпуса учебных материалов.\n",
    "\n",
    "Для достижения поставленной цели был выбран подход **Retrieval-Augmented Generation (RAG)**, сочетающий методы информационного поиска и генеративные возможности языковых моделей. Данный подход позволил обеспечить доступ к актуальным знаниям без необходимости переобучения модели, а также существенно снизить риск генерации недостоверной информации за счёт использования внешней базы знаний.\n",
    "\n",
    "В ходе работы была реализована микросервисная архитектура, включающая векторную базу данных, подсистему извлечения релевантного контекста (retriever) и сервис генерации ответов на базе языковой модели. Такое разделение ответственности позволило повысить масштабируемость системы, упростить экспериментирование с отдельными компонентами и обеспечить гибкость при дальнейшем развитии проекта.\n",
    "\n",
    "Полученные метрики решений подтверждают, что использование RAG-подхода является обоснованным и эффективным решением для построения чат-помощников в образовательных системах. Разработанная система может быть использована как основа для дальнейших исследований и расширений, включая увеличение объёма базы знаний, улучшение механизмов оценки качества ответов и адаптацию под индивидуальные потребности пользователей.\n"
   ],
   "id": "ddaf324016745e66"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
